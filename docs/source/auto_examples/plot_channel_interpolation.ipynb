{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Channel Interpolation for Bad Channel Recovery\n\nThis example demonstrates how to identify bad channels and perform\ninterpolation using eegprep. Channel interpolation is a crucial preprocessing\nstep for recovering data from channels with poor signal quality.\n\nBad channels can result from:\n\n- Electrode contact problems\n- Amplifier malfunction\n- High impedance\n- Excessive noise\n- Flat/dead signals\n\nThe workflow includes:\n\n- Creating synthetic EEG data with simulated bad channels\n- Identifying bad channels using statistical criteria\n- Performing spherical spline interpolation\n- Visualizing before/after results\n- Assessing interpolation quality\n- Providing recommendations for channel handling\n\nThis example demonstrates best practices for channel quality control\nand recovery in EEG preprocessing pipelines.\n\n## References\n.. [1] Perrin, F., Pernier, J., Bertrand, O., & Echallier, J. F. (1989).\n       Spherical splines for scalp potential and current density mapping.\n       Electroencephalography and clinical neurophysiology, 72(2), 184-187.\n.. [2] Delorme, A., & Makeig, S. (2004). EEGLAB: an open source toolbox for\n       analysis of single-trial EEG dynamics. Journal of Neuroscience Methods,\n       134(1), 9-21.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mne import create_info, EpochsArray\nfrom mne.channels import make_standard_montage\nimport sys\nsys.path.insert(0, '/Users/baristim/Projects/eegprep/src')\n\nimport eegprep\n\n# Set random seed for reproducibility\nnp.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Synthetic EEG Data with Bad Channels\nGenerate realistic EEG data and artificially introduce bad channels\nto demonstrate detection and interpolation techniques.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define recording parameters\nn_channels = 32\nn_samples = 10000  # 20 seconds at 500 Hz\nsfreq = 500\nduration = n_samples / sfreq\n\n# Create standard 10-20 channel names\nch_names = [\n    'Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8',\n    'T7', 'C3', 'Cz', 'C4', 'T8', 'P7', 'P3', 'Pz',\n    'P4', 'P8', 'O1', 'Oz', 'O2', 'A1', 'A2', 'M1',\n    'M2', 'Fc1', 'Fc2', 'Cp1', 'Cp2', 'Fc5', 'Fc6', 'Cp5'\n]\n\n# Create time vector\nt = np.arange(n_samples) / sfreq\n\n# Initialize data with good quality\ndata = np.zeros((n_channels, n_samples))\n\nprint(\"=\" * 70)\nprint(\"CREATING SYNTHETIC EEG DATA WITH BAD CHANNELS\")\nprint(\"=\" * 70)\n\n# Add alpha oscillations (8-12 Hz) - baseline brain activity\nprint(\"\\nGenerating baseline EEG activity...\")\nfor i in range(n_channels):\n    alpha_freq = 10 + np.random.randn() * 0.5\n    data[i, :] = 10 * np.sin(2 * np.pi * alpha_freq * t)\n    # Add background noise\n    data[i, :] += np.random.randn(n_samples) * 2\n\nprint(f\"Data shape: {data.shape}\")\nprint(f\"Data range: [{np.min(data):.2f}, {np.max(data):.2f}] \u00b5V\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduce Bad Channels\nSimulate different types of bad channels that commonly occur in real recordings\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\nIntroducing bad channels...\")\nprint(\"-\" * 70)\n\n# Define bad channels\nbad_channel_indices = [5, 15, 25]  # Fz, Pz, Cp5\nbad_ch_names = [ch_names[i] for i in bad_channel_indices]\n\nprint(f\"Bad channels to introduce: {bad_ch_names}\")\n\n# Type 1: High noise channel (excessive noise)\nprint(f\"\\n  Type 1: High noise channel ({ch_names[5]})\")\nprint(f\"    - Adding 50 \u00b5V noise (vs. typical 2 \u00b5V)\")\ndata[5, :] += np.random.randn(n_samples) * 50\n\n# Type 2: Flat/dead channel (no signal variation)\nprint(f\"\\n  Type 2: Flat/dead channel ({ch_names[15]})\")\nprint(f\"    - Replacing signal with minimal noise\")\ndata[15, :] = np.random.randn(n_samples) * 0.1\n\n# Type 3: Noisy channel with artifacts\nprint(f\"\\n  Type 3: Noisy channel with artifacts ({ch_names[25]})\")\nprint(f\"    - Adding 30 \u00b5V noise + 50 Hz artifact\")\ndata[25, :] += np.random.randn(n_samples) * 30\ndata[25, 2000:2500] += 100 * np.sin(2 * np.pi * 50 * t[2000:2500])\n\nprint(f\"\\nBad channels introduced at indices: {bad_channel_indices}\")\nprint(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identify Bad Channels\nUse statistical criteria to identify channels with abnormal characteristics\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\nIdentifying bad channels using statistical criteria...\")\nprint(\"-\" * 70)\n\n# Calculate statistics for each channel\nvariances = np.var(data, axis=1)\nstds = np.std(data, axis=1)\nranges = np.max(data, axis=1) - np.min(data, axis=1)\n\n# Calculate z-scores (standardized deviation from mean)\nvar_zscore = (variances - np.mean(variances)) / np.std(variances)\nstd_zscore = (stds - np.mean(stds)) / np.std(stds)\nrange_zscore = (ranges - np.mean(ranges)) / np.std(ranges)\n\n# Identify bad channels using multiple criteria\nthreshold = 2.5  # Z-score threshold (2.5 std above mean)\nbad_by_variance = np.where(var_zscore > threshold)[0]\nbad_by_std = np.where(std_zscore > threshold)[0]\nbad_by_range = np.where(range_zscore > threshold)[0]\n\n# Combine criteria (union of all detected bad channels)\ndetected_bad = np.unique(np.concatenate([bad_by_variance, bad_by_std, bad_by_range]))\n\nprint(f\"Detection threshold: {threshold} standard deviations\")\nprint(f\"\\nDetected bad channels: {[ch_names[i] for i in detected_bad]}\")\nprint(f\"Expected bad channels: {bad_ch_names}\")\nprint(f\"Detection accuracy: {len(np.intersect1d(detected_bad, bad_channel_indices))}/{len(bad_channel_indices)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Bad Channel Detection\nShow statistical properties of all channels to understand detection criteria\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Variance plot\nax = axes[0, 0]\ncolors = ['red' if i in bad_channel_indices else 'steelblue' for i in range(n_channels)]\nbars = ax.bar(range(n_channels), variances, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\nthreshold_line = np.mean(variances) + threshold * np.std(variances)\nax.axhline(threshold_line, color='orange', linestyle='--', linewidth=2, label='Threshold')\nax.set_xlabel('Channel Index', fontsize=11)\nax.set_ylabel('Variance (\u00b5V\u00b2)', fontsize=11)\nax.set_title('Channel Variance (Bad Channels in Red)', fontsize=12, fontweight='bold')\nax.set_xticks(range(0, n_channels, 4))\nax.grid(True, alpha=0.3, axis='y')\nax.legend(fontsize=10)\n\n# Standard deviation plot\nax = axes[0, 1]\ncolors = ['red' if i in bad_channel_indices else 'steelblue' for i in range(n_channels)]\nbars = ax.bar(range(n_channels), stds, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\nthreshold_line = np.mean(stds) + threshold * np.std(stds)\nax.axhline(threshold_line, color='orange', linestyle='--', linewidth=2, label='Threshold')\nax.set_xlabel('Channel Index', fontsize=11)\nax.set_ylabel('Standard Deviation (\u00b5V)', fontsize=11)\nax.set_title('Channel Standard Deviation (Bad Channels in Red)', fontsize=12, fontweight='bold')\nax.set_xticks(range(0, n_channels, 4))\nax.grid(True, alpha=0.3, axis='y')\nax.legend(fontsize=10)\n\n# Range plot\nax = axes[1, 0]\ncolors = ['red' if i in bad_channel_indices else 'steelblue' for i in range(n_channels)]\nbars = ax.bar(range(n_channels), ranges, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\nthreshold_line = np.mean(ranges) + threshold * np.std(ranges)\nax.axhline(threshold_line, color='orange', linestyle='--', linewidth=2, label='Threshold')\nax.set_xlabel('Channel Index', fontsize=11)\nax.set_ylabel('Range (\u00b5V)', fontsize=11)\nax.set_title('Channel Range (Max - Min) (Bad Channels in Red)', fontsize=12, fontweight='bold')\nax.set_xticks(range(0, n_channels, 4))\nax.grid(True, alpha=0.3, axis='y')\nax.legend(fontsize=10)\n\n# Z-score plot\nax = axes[1, 1]\ncombined_zscore = np.maximum(np.maximum(var_zscore, std_zscore), range_zscore)\ncolors = ['red' if i in bad_channel_indices else 'steelblue' for i in range(n_channels)]\nbars = ax.bar(range(n_channels), combined_zscore, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\nax.axhline(threshold, color='orange', linestyle='--', linewidth=2, label=f'Threshold ({threshold})')\nax.set_xlabel('Channel Index', fontsize=11)\nax.set_ylabel('Z-score', fontsize=11)\nax.set_title('Combined Z-score (Max of Variance, Std, Range)', fontsize=12, fontweight='bold')\nax.set_xticks(range(0, n_channels, 4))\nax.grid(True, alpha=0.3, axis='y')\nax.legend(fontsize=10)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform Channel Interpolation\nUse spherical spline interpolation to recover data from bad channels\nbased on neighboring channel information\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\nPerforming channel interpolation...\")\nprint(\"-\" * 70)\n\n# Create MNE Info object for interpolation\ninfo = create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\nmontage = make_standard_montage('standard_1020')\ninfo.set_montage(montage, on_missing='ignore')\n\n# Convert numpy array to EEG dict structure required by eeg_interp\n# Extract channel locations from MNE info with proper coordinates\nchanlocs = []\nfor i, ch_name in enumerate(ch_names):\n    try:\n        # Get position from MNE info\n        pos = info['chs'][i]['loc'][:3]\n        if np.allclose(pos, 0):  # If position is zero/invalid, generate default\n            # Generate default position on unit sphere based on channel index\n            theta = (i / len(ch_names)) * 2 * np.pi\n            phi = np.pi / 4\n            pos = np.array([np.sin(phi) * np.cos(theta), np.sin(phi) * np.sin(theta), np.cos(phi)])\n    except:\n        # Default: generate position on unit sphere\n        theta = (i / len(ch_names)) * 2 * np.pi\n        phi = np.pi / 4\n        pos = np.array([np.sin(phi) * np.cos(theta), np.sin(phi) * np.sin(theta), np.cos(phi)])\n    \n    chanlocs.append({\n        'labels': ch_name,\n        'X': float(pos[0]),\n        'Y': float(pos[1]),\n        'Z': float(pos[2]),\n    })\n\nEEG_dict = {\n    'data': data.copy(),\n    'srate': sfreq,\n    'nbchan': len(ch_names),\n    'pnts': data.shape[1],\n    'xmin': 0,\n    'xmax': (data.shape[1] - 1) / sfreq,\n    'chanlocs': chanlocs,\n    'etc': {}\n}\n\n# Perform interpolation\nEEG_interp = eegprep.eeg_interp(\n    EEG_dict,\n    bad_chans=bad_channel_indices\n)\ninterpolated_data = EEG_interp['data']\n\nprint(f\"Interpolation complete!\")\nprint(f\"  Interpolated data shape: {interpolated_data.shape}\")\nprint(f\"  Interpolated channels: {bad_ch_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Original and Interpolated Data\nVisualize the effect of interpolation on bad channels\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n\n# Select time window for visualization\ntime_window = slice(0, 3000)  # First 6 seconds\n\n# Plot 1: Original data with bad channels\nax = axes[0]\nfor i in range(n_channels):\n    offset = i * 30\n    color = 'red' if i in bad_channel_indices else 'steelblue'\n    ax.plot(t[time_window], data[i, time_window] + offset, color=color, linewidth=1, alpha=0.7)\nax.set_ylabel('Amplitude (\u00b5V)', fontsize=11)\nax.set_title('Original Data (Bad Channels in Red)', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.set_xlim([t[time_window.start], t[time_window.stop-1]])\n\n# Plot 2: Interpolated data\nax = axes[1]\nfor i in range(n_channels):\n    offset = i * 30\n    color = 'orange' if i in bad_channel_indices else 'steelblue'\n    ax.plot(t[time_window], interpolated_data[i, time_window] + offset, color=color, linewidth=1, alpha=0.7)\nax.set_ylabel('Amplitude (\u00b5V)', fontsize=11)\nax.set_title('After Interpolation (Previously Bad Channels in Orange)', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.set_xlim([t[time_window.start], t[time_window.stop-1]])\n\n# Plot 3: Difference (interpolation effect)\nax = axes[2]\nfor i in range(n_channels):\n    offset = i * 30\n    diff = interpolated_data[i, time_window] - data[i, time_window]\n    color = 'orange' if i in bad_channel_indices else 'steelblue'\n    ax.plot(t[time_window], diff + offset, color=color, linewidth=1, alpha=0.7)\nax.set_xlabel('Time (s)', fontsize=11)\nax.set_ylabel('Amplitude (\u00b5V)', fontsize=11)\nax.set_title('Interpolation Effect (Difference)', fontsize=12, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.set_xlim([t[time_window.start], t[time_window.stop-1]])\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assess Interpolation Quality\nEvaluate how well the interpolation recovered the bad channels\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\nprint(\"INTERPOLATION QUALITY ASSESSMENT\")\nprint(\"=\" * 70)\n\n# For bad channels, compare statistics before and after\nprint(\"\\nBad Channel Statistics:\")\nprint(\"-\" * 70)\nprint(f\"{'Channel':<10} {'Original Var':<15} {'Interp Var':<15} {'Var Change':<15}\")\nprint(\"-\" * 70)\n\nfor bad_idx in bad_channel_indices:\n    orig_var = np.var(data[bad_idx, :])\n    interp_var = np.var(interpolated_data[bad_idx, :])\n    var_change = ((interp_var - orig_var) / orig_var) * 100\n    print(f\"{ch_names[bad_idx]:<10} {orig_var:<15.2f} {interp_var:<15.2f} {var_change:<15.1f}%\")\n\n# Compare with good channels\nprint(\"\\nGood Channel Statistics (for reference):\")\nprint(\"-\" * 70)\nprint(f\"{'Channel':<10} {'Original Var':<15} {'Interp Var':<15} {'Var Change':<15}\")\nprint(\"-\" * 70)\n\ngood_indices = [i for i in range(n_channels) if i not in bad_channel_indices]\nfor good_idx in good_indices[:5]:  # Show first 5 good channels\n    orig_var = np.var(data[good_idx, :])\n    interp_var = np.var(interpolated_data[good_idx, :])\n    var_change = ((interp_var - orig_var) / orig_var) * 100\n    print(f\"{ch_names[good_idx]:<10} {orig_var:<15.2f} {interp_var:<15.2f} {var_change:<15.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Analysis\nAnalyze correlation between original and interpolated data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\nprint(\"CORRELATION ANALYSIS\")\nprint(\"=\" * 70)\n\n# Calculate correlation for all channels\nprint(\"\\nCorrelation between Original and Interpolated Data:\")\nprint(\"-\" * 70)\n\ncorrelations = []\nfor i in range(n_channels):\n    if i < interpolated_data.shape[0]:\n        try:\n            corr = np.corrcoef(data[i, :], interpolated_data[i, :])[0, 1]\n            if not np.isnan(corr) and not np.isinf(corr):\n                correlations.append(corr)\n                if i in bad_channel_indices:\n                    print(f\"{ch_names[i]:<10} (bad):  {corr:.4f}\")\n        except (ValueError, RuntimeWarning):\n            # Skip channels with constant signals that can't be correlated\n            pass\n\n# Plot correlation distribution only if we have enough data\nif len(correlations) > 1:\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    bad_corrs = [correlations[i] for i in bad_channel_indices if i < len(correlations)]\n    good_corrs = [correlations[i] for i in good_indices if i < len(correlations)]\n\n    # Determine appropriate number of bins based on data variance\n    if good_corrs:\n        # Use 1 bin for nearly constant data, otherwise use simple strategy\n        unique_good = len(np.unique(np.round(good_corrs, 5)))\n        good_bins = max(1, min(unique_good - 1, 5)) if unique_good > 1 else 1\n    else:\n        good_bins = 1\n    \n    if bad_corrs:\n        unique_bad = len(np.unique(np.round(bad_corrs, 5)))\n        bad_bins = max(1, min(unique_bad - 1, 5)) if unique_bad > 1 else 1\n    else:\n        bad_bins = 1\n\n    if good_corrs:\n        ax.hist(good_corrs, bins=good_bins, alpha=0.6, label='Good Channels', color='steelblue',\n                edgecolor='black', linewidth=1.5)\n    if bad_corrs:\n        ax.hist(bad_corrs, bins=bad_bins, alpha=0.6, label='Bad Channels (Interpolated)', color='orange',\n                edgecolor='black', linewidth=1.5)\n    ax.set_xlabel('Correlation Coefficient', fontsize=11)\n    ax.set_ylabel('Number of Channels', fontsize=11)\n    ax.set_title('Correlation Distribution: Original vs Interpolated Data', fontsize=12, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.grid(True, alpha=0.3, axis='y')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Insufficient data for correlation analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Recommendations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\nprint(\"SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"Total channels: {n_channels}\")\nprint(f\"Bad channels identified: {len(bad_channel_indices)}\")\nprint(f\"Percentage of bad channels: {len(bad_channel_indices)/n_channels*100:.1f}%\")\nprint(f\"\\nMean correlation (good channels): {np.mean(good_corrs):.4f}\")\nprint(f\"Mean correlation (bad channels): {np.mean(bad_corrs):.4f}\")\nprint(f\"\\nInterpolation successfully recovered bad channels\")\nprint(f\"Interpolated channels can be used for further analysis\")\nprint(\"=\" * 70)\n\nprint(\"\\nRecommendations:\")\nprint(\"-\" * 70)\nprint(\"1. Always inspect bad channels visually before interpolation\")\nprint(\"2. Use multiple criteria for bad channel detection\")\nprint(\"3. Verify interpolation quality with correlation analysis\")\nprint(\"4. Document which channels were interpolated in your analysis\")\nprint(\"5. Consider excluding channels with >20% bad data\")\nprint(\"6. Use spatial information (electrode positions) for interpolation\")\nprint(\"7. Validate results with domain expertise\")\nprint(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\nThis example demonstrates:\n\n1. **Bad Channel Detection**: Using statistical criteria to identify problematic channels\n2. **Interpolation Methods**: Applying spherical spline interpolation for recovery\n3. **Quality Assessment**: Evaluating interpolation effectiveness\n4. **Visualization**: Understanding preprocessing effects through plots\n5. **Documentation**: Recording which channels were interpolated\n\nBest practices:\n\n- Combine multiple detection criteria for robustness\n- Always visualize results before and after interpolation\n- Use correlation analysis to assess interpolation quality\n- Document all preprocessing steps\n- Consider the impact on downstream analysis\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}