{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Pierre Ablin <pierre.ablin@inria.fr>\n",
    "#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "#          Jean-Francois Cardoso <cardoso@iap.fr>\n",
    "#\n",
    "# License: BSD (3-clause)\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "from picard.densities import Tanh\n",
    "\n",
    "\n",
    "def core_picard(X, density=Tanh(), ortho=False, extended=False, m=7,\n",
    "                max_iter=500, tol=1e-7, lambda_min=0.01, ls_tries=10,\n",
    "                verbose=False, covariance=None):\n",
    "    '''Runs the Picard algorithm\n",
    "\n",
    "    The algorithm is detailed in::\n",
    "\n",
    "        Pierre Ablin, Jean-Francois Cardoso, and Alexandre Gramfort\n",
    "        Faster independent component analysis by preconditioning with Hessian\n",
    "        approximations\n",
    "        IEEE Transactions on Signal Processing, 2018\n",
    "        https://arxiv.org/abs/1706.08171\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (N, T)\n",
    "        Matrix containing the signals that have to be unmixed. N is the\n",
    "        number of signals, T is the number of samples. X has to be centered\n",
    "\n",
    "    m : int\n",
    "        Size of L-BFGS's memory. Typical values for m are in the range 3-15\n",
    "\n",
    "    max_iter : int\n",
    "        Maximal number of iterations for the algorithm\n",
    "\n",
    "    precon : 1 or 2\n",
    "        Chooses which Hessian approximation is used as preconditioner.\n",
    "        1 -> H1\n",
    "        2 -> H2\n",
    "        H2 is more costly to compute but can greatly accelerate convergence\n",
    "        (See the paper for details).\n",
    "\n",
    "    tol : float\n",
    "        tolerance for the stopping criterion. Iterations stop when the norm\n",
    "        of the gradient gets smaller than tol.\n",
    "\n",
    "    lambda_min : float\n",
    "        Constant used to regularize the Hessian approximations. The\n",
    "        eigenvalues of the approximation that are below lambda_min are\n",
    "        shifted to lambda_min.\n",
    "\n",
    "    ls_tries : int\n",
    "        Number of tries allowed for the backtracking line-search. When that\n",
    "        number is exceeded, the direction is thrown away and the gradient\n",
    "        is used instead.\n",
    "\n",
    "    verbose : boolean\n",
    "        If true, prints the informations about the algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Y : array, shape (N, T)\n",
    "        The estimated source matrix\n",
    "\n",
    "    W : array, shape (N, N)\n",
    "        The estimated unmixing matrix, such that Y = WX.\n",
    "    '''\n",
    "    # Init\n",
    "    N, T = X.shape\n",
    "    W = np.eye(N)\n",
    "    Y = X\n",
    "    s_list = []\n",
    "    y_list = []\n",
    "    r_list = []\n",
    "    signs = np.ones(N)\n",
    "    current_loss = _loss(Y, W, density, signs, ortho, extended)\n",
    "    requested_tolerance = False\n",
    "    sign_change = False\n",
    "    gradient_norm = 1.\n",
    "    if extended:\n",
    "        if covariance is None:  # Need this for extended\n",
    "            covariance = X.dot(X.T) / T\n",
    "        C = covariance.copy()\n",
    "    for n in range(max_iter):\n",
    "        # Compute the score function\n",
    "        psiY, psidY = density.score_and_der(Y)\n",
    "        # Compute the relative gradient and the Hessian off-diagonal\n",
    "        G = np.inner(psiY, Y) / T\n",
    "        del psiY\n",
    "        # Compute the squared signals\n",
    "        Y_square = Y ** 2\n",
    "        # Compute the kurtosis and update the gradient accordingly\n",
    "        if extended:\n",
    "            K = np.mean(psidY, axis=1) * np.diag(C)\n",
    "            K -= np.diag(G)\n",
    "            signs = np.sign(K)\n",
    "            if n > 0:\n",
    "                sign_change = np.any(signs != old_signs)  # noqa\n",
    "            old_signs = signs  # noqa\n",
    "            G *= signs[:, None]\n",
    "            psidY *= signs[:, None]\n",
    "            if not ortho:  # Like in extended infomax: change the gradient.\n",
    "                G += C\n",
    "                psidY += 1\n",
    "        # Compute the Hessian off diagonal\n",
    "        if ortho:\n",
    "            h_off = np.diag(G).copy()\n",
    "        else:\n",
    "            h_off = np.ones(N)\n",
    "        # Compute the Hessian approximation diagonal and regularize\n",
    "        if ortho:\n",
    "            psidY_mean = np.mean(psidY, axis=1)\n",
    "            diag = psidY_mean[:, None] * np.ones(N)[None, :]\n",
    "            h = 0.5 * (diag + diag.T - h_off[:, None] - h_off[None, :])\n",
    "            h[h < lambda_min] = lambda_min\n",
    "        else:\n",
    "            h = np.inner(psidY, Y_square) / T\n",
    "            h = _regularize_hessian(h, h_off, lambda_min)\n",
    "        del psidY, Y_square\n",
    "        # Project the gradient if ortho\n",
    "        if ortho:\n",
    "            G = (G - G.T) / 2\n",
    "        else:\n",
    "            G -= np.eye(N)\n",
    "        # Stopping criterion\n",
    "        gradient_norm = np.max(np.abs(G))\n",
    "        if gradient_norm < tol:\n",
    "            requested_tolerance = True\n",
    "            break\n",
    "        # Update the memory\n",
    "        if n > 0:\n",
    "            s_list.append(direction)  # noqa\n",
    "            y = G - G_old  # noqa\n",
    "            y_list.append(y)\n",
    "            r_list.append(1. / (np.sum(direction * y)))  # noqa\n",
    "            if len(s_list) > m:\n",
    "                s_list.pop(0)\n",
    "                y_list.pop(0)\n",
    "                r_list.pop(0)\n",
    "        G_old = G  # noqa\n",
    "        # Flush the memory if there is a sign change.\n",
    "        if extended and sign_change:\n",
    "            current_loss = None\n",
    "            s_list, y_list, r_list = [], [], []\n",
    "        # Find the L-BFGS direction\n",
    "        direction = _l_bfgs_direction(G, h, h_off, s_list, y_list, r_list,\n",
    "                                      ortho)\n",
    "        # Do a line_search in that direction:\n",
    "        converged, new_Y, new_W, new_loss, direction =\\\n",
    "            _line_search(Y, W, density, direction, signs, current_loss,\n",
    "                         ls_tries, verbose, ortho, extended)\n",
    "        if not converged:\n",
    "            direction = -G\n",
    "            s_list, y_list, r_list = [], [], []\n",
    "            _, new_Y, new_W, new_loss, direction =\\\n",
    "                _line_search(Y, W, density, direction, signs, current_loss,\n",
    "                             10, False, ortho, extended)\n",
    "        Y = new_Y\n",
    "        W = new_W\n",
    "        if covariance is not None:\n",
    "            C = W.dot(covariance).dot(W.T)\n",
    "        current_loss = new_loss\n",
    "        if verbose:\n",
    "            print('iteration %d, gradient norm = %.6g, loss = %.6g' %\n",
    "                  (n + 1, gradient_norm, current_loss))\n",
    "    infos = dict(converged=requested_tolerance, gradient_norm=gradient_norm,\n",
    "                 n_iterations=n)\n",
    "    if extended:\n",
    "        infos['signs'] = signs\n",
    "    return Y, W, infos\n",
    "\n",
    "\n",
    "def _loss(Y, W, density, signs, ortho, extended):\n",
    "    '''\n",
    "    Computes the loss function for Y, W\n",
    "    '''\n",
    "    if not ortho:\n",
    "        loss = - np.linalg.slogdet(W)[1]\n",
    "    else:\n",
    "        loss = 0.\n",
    "    for y, s in zip(Y, signs):\n",
    "        loss += s * np.mean(density.log_lik(y))\n",
    "        if extended and not ortho:\n",
    "            loss += 0.5 * np.mean(y ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _line_search(Y, W, density, direction, signs, current_loss, ls_tries,\n",
    "                 verbose, ortho, extended):\n",
    "    '''\n",
    "    Performs a backtracking line search, starting from Y and W, in the\n",
    "    direction direction. I\n",
    "    '''\n",
    "    N = W.shape[0]\n",
    "    alpha = 1.\n",
    "    if current_loss is None:\n",
    "        current_loss = _loss(Y, W, density, signs, ortho, extended)\n",
    "    for _ in range(ls_tries):\n",
    "        if ortho:\n",
    "            transform = expm(alpha * direction)\n",
    "        else:\n",
    "            transform = np.eye(N) + alpha * direction\n",
    "        Y_new = np.dot(transform, Y)\n",
    "        W_new = np.dot(transform, W)\n",
    "        new_loss = _loss(Y_new, W_new, density, signs, ortho, extended)\n",
    "        if new_loss < current_loss:\n",
    "            return True, Y_new, W_new, new_loss, alpha * direction\n",
    "        alpha /= 2.\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('line search failed, falling back to gradient')\n",
    "        return False, Y_new, W_new, new_loss, alpha * direction\n",
    "\n",
    "\n",
    "def _l_bfgs_direction(G, h, h_off, s_list, y_list, r_list, ortho):\n",
    "    q = copy(G)\n",
    "    a_list = []\n",
    "    for s, y, r in zip(reversed(s_list), reversed(y_list), reversed(r_list)):\n",
    "        alpha = r * np.sum(s * q)\n",
    "        a_list.append(alpha)\n",
    "        q -= alpha * y\n",
    "    if ortho:\n",
    "        z = q / h\n",
    "        z = (z - z.T) / 2.\n",
    "    else:\n",
    "        z = _solve_hessian(h, h_off, q)\n",
    "    for s, y, r, alpha in zip(s_list, y_list, r_list, reversed(a_list)):\n",
    "        beta = r * np.sum(y * z)\n",
    "        z += (alpha - beta) * s\n",
    "    return -z\n",
    "\n",
    "\n",
    "def _regularize_hessian(h, h_off, lambda_min):\n",
    "    discr = np.sqrt((h - h.T) ** 2 + 4. * h_off[:, None] * h_off[None, :])\n",
    "    eigenvalues = 0.5 * (h + h.T - discr)\n",
    "    # Regularize\n",
    "    problematic_locs = eigenvalues < lambda_min\n",
    "    np.fill_diagonal(problematic_locs, False)\n",
    "    i_pb, j_pb = np.where(problematic_locs)\n",
    "    h[i_pb, j_pb] += lambda_min - eigenvalues[i_pb, j_pb]\n",
    "    return h\n",
    "\n",
    "\n",
    "def _solve_hessian(h, h_off, G):\n",
    "    det = h * h.T - h_off[:, None] * h_off[None, :]\n",
    "    return (h.T * G - h_off[:, None] * G.T) / det\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1, gradient norm = 2.85029, loss = 12.9391\n",
      "iteration 2, gradient norm = 3.35743, loss = 12.6911\n",
      "line search failed, falling back to gradient\n",
      "iteration 3, gradient norm = 4.29822, loss = 11.054\n",
      "iteration 4, gradient norm = 1.37362, loss = 10.5715\n",
      "iteration 5, gradient norm = 1.48094, loss = 10.1934\n",
      "iteration 6, gradient norm = 1.64054, loss = 10.1333\n",
      "line search failed, falling back to gradient\n",
      "iteration 7, gradient norm = 1.55536, loss = 9.50678\n",
      "iteration 8, gradient norm = 1.43082, loss = 8.84081\n",
      "iteration 9, gradient norm = 0.979654, loss = 8.53688\n",
      "iteration 10, gradient norm = 1.50304, loss = 8.46077\n",
      "line search failed, falling back to gradient\n",
      "iteration 11, gradient norm = 1.49455, loss = 7.72602\n",
      "iteration 12, gradient norm = 0.754297, loss = 7.67735\n",
      "iteration 13, gradient norm = 0.772179, loss = 7.06625\n",
      "iteration 14, gradient norm = 0.886635, loss = 7.04377\n",
      "iteration 15, gradient norm = 0.884111, loss = 6.99638\n",
      "iteration 16, gradient norm = 0.734814, loss = 6.972\n",
      "iteration 17, gradient norm = 0.829269, loss = 6.91114\n",
      "line search failed, falling back to gradient\n",
      "iteration 18, gradient norm = 0.814726, loss = 6.66599\n",
      "iteration 19, gradient norm = 0.435437, loss = 6.57359\n",
      "iteration 20, gradient norm = 0.561115, loss = 6.54909\n",
      "iteration 21, gradient norm = 0.521726, loss = 6.53167\n",
      "iteration 22, gradient norm = 0.426887, loss = 6.50874\n",
      "iteration 23, gradient norm = 0.441002, loss = 6.47984\n",
      "iteration 24, gradient norm = 0.437988, loss = 6.45726\n",
      "iteration 25, gradient norm = 0.480014, loss = 6.43252\n",
      "iteration 26, gradient norm = 0.479211, loss = 6.38526\n",
      "iteration 27, gradient norm = 0.360861, loss = 6.37245\n",
      "iteration 28, gradient norm = 0.443723, loss = 6.30739\n",
      "iteration 29, gradient norm = 0.330491, loss = 6.2707\n",
      "iteration 30, gradient norm = 0.462104, loss = 6.23688\n",
      "iteration 31, gradient norm = 0.339772, loss = 6.18056\n",
      "iteration 32, gradient norm = 0.253634, loss = 6.14288\n",
      "iteration 33, gradient norm = 0.222949, loss = 6.10836\n",
      "iteration 34, gradient norm = 0.177148, loss = 6.07476\n",
      "iteration 35, gradient norm = 0.141939, loss = 6.04915\n",
      "iteration 36, gradient norm = 0.111286, loss = 6.02712\n",
      "iteration 37, gradient norm = 0.0696807, loss = 6.02209\n",
      "iteration 38, gradient norm = 0.0690637, loss = 6.01833\n",
      "iteration 39, gradient norm = 0.021205, loss = 6.01785\n",
      "iteration 40, gradient norm = 0.0128834, loss = 6.01767\n",
      "iteration 41, gradient norm = 0.00456482, loss = 6.01764\n",
      "iteration 42, gradient norm = 0.00133094, loss = 6.01764\n",
      "iteration 43, gradient norm = 0.000443109, loss = 6.01763\n",
      "iteration 44, gradient norm = 7.11577e-05, loss = 6.01763\n",
      "iteration 45, gradient norm = 6.32613e-06, loss = 6.01763\n",
      "Python converged in 45 iterations\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import savemat, loadmat\n",
    "from picard import _core_picard\n",
    "    \n",
    "def amari_index(W_true, W_est):\n",
    "    P = W_est @ np.linalg.inv(W_true)\n",
    "    C = np.abs(P)\n",
    "    row_sum = np.sum(C, axis=1, keepdims=True)\n",
    "    col_sum = np.sum(C, axis=0, keepdims=True)\n",
    "    r = np.sum((C / row_sum - 1/P.shape[1])**2)\n",
    "    c = np.sum((C / col_sum - 1/P.shape[0])**2)\n",
    "    return (r + c) / (2 * P.shape[0])\n",
    "\n",
    "# fixed seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate sources\n",
    "N, T = 5, 10000\n",
    "S = np.random.laplace(size=(N, T))\n",
    "\n",
    "# random mixing\n",
    "A = np.random.randn(N, N)\n",
    "X = A @ S\n",
    "\n",
    "# save data for MATLAB\n",
    "savemat('picard_data.mat', {'X': X, 'A': A})\n",
    "\n",
    "# run Python Picard\n",
    "# from threadpoolctl import threadpool_limits\n",
    "#with threadpool_limits(limits=1, user_api=\"blas\"):\n",
    "Y_py, W_py, info_py = core_picard(X.copy(), ortho=False, extended=False,\n",
    "                                  max_iter=200, tol=1e-6, m=10,\n",
    "                                  lambda_min=0.01, ls_tries=10, verbose=True)\n",
    "print('Python converged in', info_py['n_iterations'], 'iterations')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
